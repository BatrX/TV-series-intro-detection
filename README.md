# Детекция заставок в сериалах с использованием CLIP и трансформеров

## Описание задачи

Цель проекта — автоматическое определение коротких **заставок (интро)** в эпизодах сериалов. Эти фрагменты, как правило, являются повторяющимися вступительными титрами или логотипами. Задача решается как **покадровая бинарная классификация**: для каждого кадра нужно определить, относится ли он к заставке (`1`) или нет (`0`).

## Основа подхода

За основу взята статья:  
[**Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention**](https://arxiv.org/abs/2504.09738)

Однако:
- В оригинальной работе определяются **и интро, и титры**, а в данной задаче — **только интро**.
- Исходный код статьи отсутствует, поэтому многие решения приходилось **интерпретировать самостоятельно** (в частности, как формировать сбалансированные последовательности, как обращаться с короткими интро и т.д.).

## Пайплайн обработки

### 1. Обработка видео

- Все видео приводятся к **1 кадру в секунду (1 FPS)**. Это соответствует статье — как **оптимальный компромисс между точностью и вычислительными затратами**.
- Используется `.json` файл с аннотациями: `start` и `end` времени заставки для каждого эпизода.

### 2. Исправление ошибок разметки

- В аннотациях часто начало заставки оказывалось **на 60 секунд позже** её конца — это было **автоматически исправлено**.
- Некоторые другие ошибки (рандомные) остались, так как их исправление требовало ручной обработки.

### 3. Формирование последовательностей

- Поскольку заставка занимает **очень малый процент** от общего видео, для сбалансированного обучения:
  - Из каждого видео берётся сегмент заставки.
  - К нему добавляется **фильм-сегмент той же длины**, следующий за заставкой.
- Все такие пары (заставка + фильм) из всех видео **объединяются в одну большую последовательность**.
- Размер итоговой последовательности:
  - Трейн: 2068 кадров
  - Тест: 992 кадра

- Эта последовательность **разбивается на отрезки по 60 кадров со страйдом 5**, чтобы:
  - Захватывать **темпоральную структуру**
  - Создать больше обучающих примеров

- Результат:
  - `train_clip_sequences`: 402 последовательности
  - `test_clip_sequences`: 189 последовательностей

### 4. Извлечение признаков (CLIP)

- Каждый кадр пропускается через **CLIP ViT-B/32**.
- Получается **512-мерный эмбеддинг** для каждого кадра.
- Итог: вход в модель имеет размер `[60, 512]` (последовательность из 60 фреймов по 512 признаков).

## Архитектура модели

```python
Input: [batch_size, 60, 512]
→ + обучаемые позиционные эмбеддинги [1, 60, 512]
→ TransformerEncoder (16 слоёв, 16 голов)
→ Linear(512 → 1) → по каждому кадру логит


